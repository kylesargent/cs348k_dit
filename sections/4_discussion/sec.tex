\section{Discussion}

% Did we improve upon anything?

% What should we conclude?

% Mamba is not a magic bullet

% Specifiic details of the implementation are extremely important

% For example, attention implementations have had years to mature
% xformers and fast attention
% Perhaps alternative implementations just require more time

% However, there remains ample room to improve upon diffusion transformers